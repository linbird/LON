[]`<!-- vim-markdown-toc GFM -->

+ [进程管理](#进程管理)
    * [进程](#进程)
    * [线程](#线程)
        - [内核线程/守护进程](#内核线程守护进程)
            + [轻量级进程](#轻量级进程)
            + [优点](#优点)
            + [缺点](#缺点)
        - [用户线程](#用户线程)
            + [用户线程的实现](#用户线程的实现)
                * [运行时系统](#运行时系统)
                * [内核控制线程](#内核控制线程)
            + [优点](#优点-1)
            + [缺点](#缺点-1)
        - [线程实现模型](#线程实现模型)
            + [用户线程:内核线程 = 1:1](#用户线程内核线程--11)
                * [优点](#优点-2)
                * [缺点](#缺点-2)
            + [混合线程模型](#混合线程模型)
            + [**混合线程模型** 用户线程:内核线程 = N : 1](#混合线程模型-用户线程内核线程--n--1)
                * [优点](#优点-3)
                * [弊端](#弊端)
            + [**混合线程模型** 用户线程:内核线程 = N : M](#混合线程模型-用户线程内核线程--n--m)
                * [优点](#优点-4)
                * [弊端](#弊端-1)
        - [线程调度](#线程调度)
        - [线程同步（锁）](#线程同步锁)
            + [互斥锁](#互斥锁)
            + [读写锁](#读写锁)
            + [条件变量](#条件变量)
            + [自旋锁](#自旋锁)
            + [内存屏障](#内存屏障)
    * [现代OS下的调度](#现代os下的调度)
    * [协程](#协程)
        - [优点](#优点-5)
+ [内存管理](#内存管理)
    * [分页管理](#分页管理)
    * [分段管理](#分段管理)
+ [文件管理](#文件管理)
    * [基本概念](#基本概念)
        - [超级块](#超级块)
        - [索引节点 INode](#索引节点-inode)
        - [文件](#文件)
    * [进程对文件的管理](#进程对文件的管理)
+ [Linux Kernel](#linux-kernel)
    * [设计哲学](#设计哲学)
        - [一切皆文件](#一切皆文件)
    * [系统调用](#系统调用)
    * [虚拟文件系统VFS](#虚拟文件系统vfs)
        - [VFS数据结构](#vfs数据结构)
            + [超级块对象](#超级块对象)
            + [索引节点对象](#索引节点对象)
            + [目录项对象](#目录项对象)
            + [文件对象](#文件对象)
            + [NOTE](#note)
        - [tmpfs](#tmpfs)
        - [sysfs](#sysfs)
        - [debugfs](#debugfs)
        - [procfs](#procfs)
        - [sockfs](#sockfs)
        - [devfs](#devfs)
    * [调度管理](#调度管理)
        - [Linux的任务管理](#linux的任务管理)
        - [Linux线程](#linux线程)
            + [共享的资源](#共享的资源)
            + [线程独有](#线程独有)
        - [多线程编程模式](#多线程编程模式)
            + [leader-follow 模型（主从）](#leader-follow-模型主从)
            + [producer-consumer模型（生产者消费者）](#producer-consumer模型生产者消费者)
            + [高并发索引模型](#高并发索引模型)
        - [相关问题](#相关问题)
+ [内核态与用户态](#内核态与用户态)
    * [内存布局](#内存布局)
    * [用户态与内核态](#用户态与内核态)
        - [核心态](#核心态)
        - [用户态](#用户态)
        - [NOTE](#note-1)
    * [上下文切换](#上下文切换)

<!-- vim-markdown-toc -->

# 进程管理

## 进程

+ 一个包含线程集和资源集的动态实体（程序运行时的产物），是**资源管理及分配的最小单元**

+ **NOTE**：用户进程同时具备内核空间与用户空间，在进行系统调用时用户进程会由用户内存空间陷入内核内存空间。

### 进程状态

![进程状态转换图](img/OS/process-state.webp)

* 挂起：此时进程不占用内存空间，此时进程所使用的空间在硬盘而非物理内存上。其中就绪挂起状态的进程一旦进入内存就可以运行。

## 线程

+ 程序**执行的最小单元**

|      |
| ---- |

![线程](img/OS/thread.webp)

|                      |             内核线程             |                   用户线程                   |
| :------------------: | :------------------------------: | :------------------------------------------: |
|      OS内核感知      |              可感知              |                    无感知                    |
| 管理主体(建立和销毁) |             操作系统             | 线程库而非内核支持（创建、同步、调度和管理） |
|      处理器分配      | 进程的多个线程可以获得多个处理机 |                   单处理器                   |
|      OS调度单位      |               线程               |       内核调度进程、进程自定义线程调度       |
|        工作态        |            只在内核态            |        用户态和内核态(执行系统调用时)        |

### 内核线程/守护进程

+ 每一个内核线程都是内核部分代码的一个运行实体，相当于以一个特化的部分，每个内核线程都有其特定的任务

+ 内核为每一个内核线程维护一个内核控制块TCB，通过TCB感知和调度内核线程。

+ 内核线程是内核调度的基本单位，是“独立运行在内核空间的标准进程”。每一个内核线程都可以在全系统内进行资源的竞争。

+ 内核线程没有自己的地址空间，与内核使用同一张页表

+ 以下是一些特殊的内核线程（Linux下）

   | 内核线程 |                           任务                           |
   | :------: | :------------------------------------------------------: |
   |   `init`   | 运行文件系统上的一系列”init”脚本，并启动shell进程；pid=1 |
   | `kthreadd` |    内核的守护线程，在内核正常工作时永远不退出；pid=2     |
   |`kswapd`|在内存不足时将内存页写回磁盘|
   | `kflushd、pdflush` |周期性的将脏数据写回磁盘|


#### 优点

+ 在多处理器系统中，内核能够同时调度同一进程中多个线程并行执行到多个处理器中；
+ 如果进程中的一个线程被阻塞，内核可以调度同一个进程中的另一个线程；
+ 内核支持线程具有很小的数据结构和堆栈，内核线程之间的切换快、开销小；
+ 内核本身也可以使用多线程的方式来实现（如Linux的kswapd线程负责在内存不足时将内存页写回磁盘、kflushd、pdflushd线程负责周期性的将脏数据写回磁盘）。

#### 缺点

+ 即使CPU在同一个用户进程的多个线程之间切换，也需要陷入内核，因此其速度和效率不如用户级线程。
+ 当线程进行切换的时候，由用户态转化为内核态。切换完毕要从内核态返回用户态?????????

### 轻量级进程

建立在内核之上并有**内核支持的用户线程**，**每个LWP跟内核线程一对一映射的，一个进程可有一个或多个 LWP**。只能由**内核管理**并像普通进程一样被调度。

LWP与普通进程的区别也在于它只有一个**最小的执行上下文和调度程序所需的统计信息**。一个进程代表程序的一个实例，而 LWP 代表程序的执行线程，因为一个执行线程不像进程那样需要那么多状态信息，所以 LWP 也不带有大量的信息。

### 用户线程

+ 不需要内核支持而在用户空间中实现的线程，用户进程利用线程库提供创建、同步、调度和管理线程的函数来控制用户线程。其内部的活动对于内核是透明的。
+ 每个进程里的**线程表**由运行时系统管理。当一个线程转换到就绪状态或阻塞状态时，在该线程表中存放重新启动该线程所需的信息

#### 用户线程的实现

用户线程运行在一个中间系统上面。目前中间系统实现的方式有两种，即运行时系统（Runtime System）和内核控制线程。

##### 运行时系统

+ “运行时系统”实质上是用于**管理和控制线程的函数集合**，包括创建、撤销、线程的同步和通信的函数以及调度的函数。
+ 这些函数都驻留在用户空间作为用户线程和内核之间的接口。
+ 当线程需要系统资源时不能使用系统调用，而是将请求传送给运行时，由后者通过相应的系统调用来获取系统资源。

##### 内核控制线程

+ 系统构建若干个轻型进程（LWP）形成线程池，LWP可以通过系统调用来获得内核提供的服务，而进程中的用户线程可通过复用来关联到线程池中的LWP，从而得到内核的服务。

  ![轻量级进程](./img/OS/User-LWP-Kernal.jpg)

#### 优点

1. 可以在不支持线程的OS上实现线程
2. 不需要内核干涉，少了进出内核态的消耗，管理（创建、切换、销毁）线程的代价比进程小得多
3. 进程可以定制自己的线程的调度策略（进程内部没有时钟中断，所以不能用轮转调度的方式），管理灵活度高
4. 能够利用的表空间和堆栈空间比内核级线程多
5. 不需要陷阱，不需要上下文切换，也不需要对内存高速缓存进行刷新，使得线程调用非常快捷

#### 缺点

1. 同一进程同时只有一个线程运行，同一进程的多线程无法利用多处理机并行
2. 操作系统内核不知道多线程的存在，因此一个线程阻塞将使得整个进程（包括它的所有线程）阻塞。


### 线程实现模型

内核态的系统线程专⻔负责执⾏，⽽⽤户态的线程负责存储状态（线程栈状态、寄存器相关的信息、局部变量等）。内核将若干个内核态的线程构成内核态线程池pool，⽤户态只需要创建抽象的专⻔⽤来存储状态的这种⽤户态线程，当用户态线程需要执⾏的时候，将它绑定到⼀个系统线程上由系统线程去执⾏，当执⾏完了以后将系统线程释放回Pool⾥而不需要消灭这个系统线程。[参考来源](https://www.jianshu.com/p/49e3e47d41f0)

#### 用户线程:LWP = 1:1

+ 操作系统调度器管理、调度并分派这些线程。运行时库为每个用户级线程请求一个内核级线程。

+ 一个用户线程在其生命周期内被映射/绑定到一个内核线程、每个用户线程都对应一个内核线程作为调度实体(反过来不一定成立，一个内核线程**不一定有**对应的用户线程)。

+ 内核会对每个线程进行调度，所以线程在用户空间的切换就涉及到了多个内核态的线程的切换。

+ 创建方法：一般一直使用API或者通过系统调用(Linux:clone、Windows:CreateThread)创建的线程为一对一模型。

![1:1模型](./img/OS/1kernel:1user.jpg)

##### 优点

1. 一个线程因某种原因阻塞时其他线程的执行不受影响
2. 多线程的程序在多处理机上能够充分利用多个处理机，提高程序的表现。

##### 缺点

1. 内核支持的内核线程数量有限，许多操作系统限制了内核线程的数量。
2. OS在内核线程之间的调度时开销比较大

#### 混合线程模型

+ 用户线程库和内核都可以参与线程的管理，用户线程由运行时库调度器管理，内核线程由操作系统调度器管理。
+ 准备就绪的用户线程由运行时库分派并标记为可执行，操作系统选择可执行的用户线程并将它映射到线程池中的可用内核线程。

#####  用户线程:LWP = N : 1

+ 将多个用户线程映射到一个内核线程，线程的创建、调度、同步的所有细节全部由进程的用户空间线程库来处理。

![N:1模型](./img/OS/1kernel:Nuser.jpg)

##### 优点

1. 对用户线程的数量几乎无限制。
2. 线程之间的切换由用户态的代码来进行，相对一对一模型其线程切换速度要快许多；

##### 弊端

1. 如果其中一个用户线程阻塞将导致其绑定的内核线程阻塞，绑定到该内核线程的其他用户线程也会阻塞。
2. 在多处理器系统上，处理器数量的增加对多对一模型的线程性能不会有明显的增加，因为所有的用户线程都映射到一个处理器上了。

##### 用户线程:LWP = N : M

+ 将多个用户线程映射到多个内核线程上。
+ 是实现原生协程的关键
![N:M模型](./img/OS/Mkernel:Nuser.jpg)

##### 优点

1. 一个用户线程的阻塞不会导致所有线程的阻塞，因为此时还有别的内核线程被调度来执行；
2. 对用户线程的数量没有限制；

##### 弊端

1. 多处理机下的性能提升不如1:1模型提升大

## 协程

+ 协程更多的是一种暂停的概念。在一个线程中可以通过协调器来暂停继续不同的协程而避免使用线程的上下文切换，从而实现不同协程的交替运行。以上所有操作都在用户态执行，开销小。

### 优点

1. 协程由用户自己进行调度，因此减少了上下文切换，提高了效率。
2. 线程的默认Stack大小是1M，而协程更轻量，接近1K。因此可以在相同的内存中开启更多的协程。
3. 由于在同一个线程上，因此可以避免竞争关系而使用锁。
4. 适用于被阻塞的、IO频繁且需要大量并发的场景。

## 调度算法

在同一进程中，线程的切换不会引起进程切换。在不同进程中进行线程切换,如从一个进程内的线程切换到另一个进程中的线程时，会引起进程切换。


### FCFS 先来先服务

非抢占式调度算法，对长作业有利，适用于 CPU 繁忙型不适用于 I/O 繁忙型系统。

### SJF 最短作业优先

### 高响应比优先

权衡了短作业和长作业，每次进行进程调度时，先计算响应比（$(T_{等待}+T_{要求服务})/T_{要求服务}$），然后把响应比优先级最高的进程投入运行。

### RR 时间片轮转

抢占式调度算法，简单公平，关键在于时间片长度的选择（一般为20-50ms）

### HPF 最高优先级优先

其中优先级可以分为动态优先级和静态优先级，既可以抢占式调度（出现高优先级则停止此进程）也可以非抢占式调度。

### MFQ 多级反馈队列

* **多级**：有多个不同优先级的队列，优先级越高时间片越短。

* **反馈**：若有新进程进入高优先级队列，则处理机被抢占执行高优先级队列中的进程

  ![多级反馈队列](img/OS/MFQ.webp)

1. 新的进程会被放入到第一优先级队列的末尾，按**先来先服务**的原则排队等待被调度。如果当前优先队列队头的进程在该优先级队列规定的时间片没运行完成，则将其转入到下一优先级队列的末尾，以此类推，直至完成；

2. **当较高优先级的队列为空，才调度较低优先级的队列中的进程运行**。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；

对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也会更长，所以该算法很好的**兼顾了长短作业，同时有较好的响应时间。**

## 同步（锁）

### 互斥锁

### 读写锁

### 条件变量

### 自旋锁

### 内存屏障

# 内存管理

## 虚拟内存

不管是用户空间还是内核空间，使用的地址都是虚拟地址。

虚拟内存管理采用**按需分配 + 缺页异常**机制来管理页表项和分配对应的物理内存页。进程申请了虚拟地址空间后可能不会马上占用物理内存空间，只有进程实际访问内存的时候，当一个虚拟地址对应的页表项不存在时，才会由内核的请求分页机制先创建页表结构，再分配物理内存页，再修改页表，建立页表项的映射关系。

进程的虚拟地址空间由多个虚拟内存区域（即不同性质的段）构成。Linux内核使用**`vm_area_struct`结构**（包含区域起始和终止地址、指向该区域支持的系统调用函数集合的**`vm_ops`指针**）来表示一个独立的虚拟内存区域，一个进程拥有的多个`vm_area_struct`将被连接起来方便进程访问。

![Linux进程内存管理](img/OS/vm_area_struct.png)

## 内存管理单元 MMU

MMU有时称作分页内存管理单元（PMMU，因为目前普遍采用了分页思想），是为了满足OS复杂的地址管理而产生的一个与软件密切相关的**硬件**。它透明的向上层提供虚实地址转换、内存保护、CPU缓存控制等功能。

![](img/OS/TLB-MMU.webp)

### 地址转换

#### 物理地址空间

**包括物理内存地址空间和IO地址空间**，物理空间的大小和地址总线的长度相关，可以远远大于DRAM的实际大小。

![物理地址空间](img/OS/PA-Space.png)

##### [IO地址空间](https://blog.csdn.net/Buyi_Shizi/article/details/51094736?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control)

X86一个特有的与内存空间独立的空间。**可以通过利用IO空间和内存空间两种方式操作数据**。当当外部寄存器或内存映射到IO空间时只能使用对应的IO端口操作函数操作数据例（如`inb(), inbw(), inl(); outb(), outw(), outl()`等）。

#### 虚拟地址空间

虚拟地址的空间**和[指令集的地址长度有关，不一定和物理地址长度一致](https://www.cnblogs.com/doggod/p/13403622.html)**，大多数的64位处理器（数据总线一定64b）地址总线都小于64位。

#### 转换

![物理空间与虚拟空间的对应](img/OS/NVA-1PA.png)

每个进程有自己的虚拟空间，这些虚拟空间可以映射到物理内存的不同或者相同的位置。进程使用的是虚拟地址空间之内的地，CPU在执行时产生的地址信号（虚存VA）再被发送到实际的物理硬件之前会被CPU 芯片中的**内存管理单元（MMU）**截获，MMU会负责**把VA翻译成物理地址（PA）**然后物理部件上以获取数据。


### 内存保护

MMU会检查PT中VA对应的PTE中的访问权限（PTE中的一个域），如果访问不符合限定就中止转换并抛出代表异常的信号。

### MMU与OS的配合

1. 系统初始化代码会在内存中**生成页表**，然后把页表地址**设置给MMU对应寄存器**，使MMU知道页表在物理内存中的什么位置，以便在需要时进行查找。
2. 之后通过专用指令**启动MMU**，之后MMU硬件开始自动完成查表和虚实地址转换，程序中所有内存地址都变成虚地址。
3. OS初始化完成后创建**第一个用户进程**，此过程中也要创建页表，并把页表地址赋值给PCB中的某指针成员。
4. 用户**创建子进程**时会**拷贝父进程的页表**，并在随后的运行过程中逐渐更新页表项。即**每个进程都有自己的页表**。

## 多级cache

为了弥补CPU与内存（由DRAM构成）两者之间的速度差异，就在CPU内部引入了CPU Cache（也称高速缓存，由SRAM构成）。Cache通常分为大小不等的三级缓存，其中L1 Cache 通常会分为**数据缓存和指令缓存**， **L3 Cache由多个核心共享**。
![一种Cache架构图](img/OS/cache-layer.webp)

![访问速度](img/OS/access-speed.webp)

#### Cache管理

CPU**以Cache Line而非字节**作为Cache的基本管理单位，在读取数据的时候CPU永远**先访问 Cache**，当 Cache 中找不到数据时才会去访问内存（顺序加载内存地址开始的一个Cache Line长度），并把内存中的数据读入到 Cache 中，再从 CPU Cache读取数据，CPU从Cache读取数据时又以**字Word**为基本单位，因此访问地址中必然包含**偏移量Offset**字段（索引字在Line中的位置）。操作系统为了方便对内存和Cache进行管理，会以**内存块**（大小等于Cache Line）为基本单位管理内存，建立内存块与Cache行的映射关系。

![Cache管理](img/OS/cache-prociy.png)

#### 直接映射

把内存块的地址始终映射在一个 CPU Line的地址（多使用地址取模）。此时内存块和Cache Line构成**多对一**的关系，因此需要在Cache Line中加入组**标记Tag**（区别同一Cahe Line对应的不同内存块），此外还需要加入**有效位Valid**（数据是否是有效）。![直接映射内存访问](img/OS/direct-connect.webp)

#### 全相联映射

主存中任何一块都可以映射到Cache中的任何一块位置上，此时内存块和Cache Line构成**多对多**的关系（设计复杂，适合小容量Cache）。

#### 组相联映射

主存和Cache都**分组**，主存中一个**组内的块数**与Cache中的**分组数**相同，组间采用直接映射，组内采用全相联映射。![组相联映射](img/OS/Set-Associative.png)

## 分页管理

### 管理基础

**页Page**是VA到PA映射过程中的最小单位（一般为4K），整个虚拟和物理内存空间都切成若干个页；**页表PT**是为了辅助MMU完成地址映射的软件构造出的记录集合；集合中的每一个条目（一般4B）为代表映射规则的**页表项PTE**，整个**页表保存在片外内存DRAM**；为了避免每次转换都要访问片外内存以加快地址转换，MMU在内部置有SRAM来缓存页表访问记录，这个缓存表称为**页表缓存TLB**。

![分页寻址](img/OS/VA2PA-page-1.webp)

**多级页表**：多级页表最终的映射粒度是页，但是每一级页表的映射单位是下一级页表映射的范围总和，这样既可以减少页表的大小，将地址的查询分级进行（减少查找空间以加速查找），又可以灵活控制每一级的映射粒度。多级页表的关键在于并**不需要为一级页表中的每一个 PTE  都分配一个二级页表**，而只需要为进程当前使用到的地址做相应的分配和映射。因此，对于大部分进程来说，它们的一级页表中有大量空置的 PTE，那么这部分 PTE 对应的二级页表也将无需存在，这是一个相当可观的内存节约，事实上对于一个典型的程序来说，理论上的 4GB  可用**虚拟内存地址空间绝大部分都会处于这样一种未分配的状态**；更进一步，在程序运行过程中，只需要把一级页表放在主存中，虚拟内存系统可以在实际需要的时候才去创建、调入和调出二级页表，这样就可以确保只有那些最频繁被使用的二级页表才会常驻在主存中，此举亦极大地缓解了主存的压力。

![多级页表减少主存占用](img/OS/multi-level-page-table.webp)

![分页寻址](img/OS/MPT.webp)

### 页寻址

1. 处理器生成一个虚拟地址 VA，通过总线发送到 MMU；

2. MMU 通过虚拟页号得到页表项的地址 PTEA，通过内存总线从 CPU 高速缓存/主存读取这个页表项 PTE；

3. CPU 高速缓存或者主存通过内存总线向 MMU 返回页表项 PTE；

4. MMU 先把页表项中的物理页框号 PPN 复制到寄存器的高三位中，接着把 12 位的偏移量 VPO 复制到寄存器的末 12 位构成 15 位的物理地址，即可以把该寄存器存储的物理内存地址 PA 发送到内存总线，访问高速缓存/主存；

5. CPU 高速缓存/主存返回该物理地址对应的数据给处理器

   ![分页寻址](img/OS/VA2PA-via-page2.webp)

![分页寻址](img/OS/VA2PA-via-page1.webp)

### 页面置换（请求分页）

在 MMU 进行地址转换时，如果页表项的有效位是 0，则表示该页面并没有映射到真实的物理页框号 PPN，则会引发一个**缺页中断**，CPU 陷入操作系统内核，接着操作系统就会通过页面置换算法选择一个页面将其换出  (swap)，以便为即将调入的新页面腾出位置，如果要换出的页面的页表项里的修改位已经被设置过，也就是被更新过，则这是一个**脏页** (Dirty  Page)，需要写回磁盘更新该页面在磁盘上的副本，如果该页面是"干净"的，也就是没有被修改过，则直接用调入的新页面覆盖掉被换出的旧页面即可。**缺页时未必发生页面置换，若还有可用的空闲内存空间就不用进行页面置换**。

#### 缺页中断

4. 检查返回的页表项 PTE 发现其有效位是 0，则 MMU 触发一次缺页中断异常，然后 CPU 转入到操作系统内核中的缺页中断处理器；
5. 缺页中断处理程序检查所需的虚拟地址是否合法，确认合法后系统则检查是否有空闲物理页框号 PPN 可以映射给该缺失的虚拟页面，如果没有空闲页框，则执行页面置换算法寻找一个现有的虚拟页面淘汰，如果该页面已经被修改过，则写回磁盘，更新该页面在磁盘上的副本；
6. 缺页中断处理程序从磁盘调入新的页面到内存，更新页表项 PTE；
7. 缺页中断程序返回到原先的进程，**==重新执行引起缺页中断的指令==**，CPU 将引起缺页中断的虚拟地址重新发送给 MMU，此时该虚拟地址已经有了映射的物理页框号 PPN，因此会按照前面命中的流程走一遍，最后主存把请求的数据返回给处理器。

![缺页异常](img/OS/page-exception.webp)

#### 置换算法

|       算法        |              说明              |     优点     |               缺点                |
| :---------------: | :----------------------------: | :----------: | :-------------------------------: |
|    最佳置换OPT    |       无法实现、仅做基准       |     最优     |              乌托邦               |
|   先进先出FIFO    |   每次淘汰最早进入内存的页面   |     简单     | 性能差、违背局部性、造成**Belay** |
|   第二次机会SC    |           FIFO的改进           |              |          需要记录访问位           |
|       Clock       |            SC的改进            |              |       未考虑页面是否被修改        |
| 最近最久未使用LRU | 需要记录每个页面的上次访问时间 |   接近OPT    | 需硬件支持、未考虑页面是否被修改  |
|    最近未用NRU    |   加入修改位，LRU算法的近似    | 延后脏页写回 |           考虑页面修改            |
|        NFU        |       LRU的一种软件实现        |              |        未考虑最近访问特征         |
|   Aging老化算法   | NFU的改进，非常近似于LRU的算法 |              |                                   |

**Belay现象**：如果对一个进程未分配它所要求的全部页面，有时就会出现分配的**页面数增多但缺页率反而提高**的异常现象。

**抖动**：如果分配给进程的存储块数量小于进程所需要的最小值，进程的运行将很**频繁地产生缺页中断并换入换出**，这种频率非常高的页面置换现象称为抖动。

##### SC（Second Chance）

对FIFO的改进，考虑到了页面是否被访问。在将页面换出内存前先检查其访问位，如果为0则直接换出，为1（最近有被使用，可能还会被使用）则置为0后将该页面**插入到链表尾部**。再检查下一个页面，直到发现某页的使用位为0。

##### Clock

对SC算法的改进，将内存中的页面组织成**环形链表**，避免将页面插入到链表尾部。表头指向最先进入内存的页面。当页面被装入内存时**初始化**其对应页表项的访问位为0，当该页面被访问时访问位设为1。发生缺页中断时，遍历链表直到找到访问位为0的页面淘汰该页面，遍历过程中访问位为1的设置为0。该算法**最多遍历两次**就可以找到被淘汰的页面。

##### LRU （Least Recently Used）

最近最久未使用（多译为最近最少使用），理论基础是访问的局部性原理（在前面几条指令频发访问的页面很可能在后面的若干指令也可能会被访问）。

##### NRU （Not Recently Used）

使用访问位+修改位，是LRU算法的粗糙的近似实现。在其他在条件相同时优先淘汰没有被修改过的页面，**从而来避免I/O操作**。该方法最多经过四轮可以找到需要淘汰的页。

1. 从当前位置开始扫描第一个（0,0）的页用于替换，本轮扫描不修改任何标志位。淘汰的是**最近没有访问且没有修改**的页面。

2. 若第一轮扫描失败，则重新扫描，查找第一个（0,1）的页用于替换。本轮将所有扫描的过的页访问位设为0。淘汰的是**最近没有访问但修改**的页面。

3. 若第二轮扫描失败，则重新扫描，查找第一个（0,0）的页用于替换。本轮扫描不修改任何标志位。淘汰的是**最近访问但没有修改**的页面。

4. 若第三轮扫描失败，则重新扫描，查找第一个（0,1）的页用于替换。淘汰的是**最近访问且修改**的页面。

#####  NFU（Not Frequently Used）

LRU算法的一种软件实现。该算法将每个页面和一个**计数器**相关联，每次时钟中断时，操作系统扫描所有页面的R位（0 or 1）并把它们累加到计数器，该**计数器就大致体现了页面的被访问的频率程度**。在发生缺页中断时就置换计数器值最小的算法。但是计数器反映的是过去整个时间的访问频率而非最近时间段的访问频率。

##### Aging

对NFU算法的改进。在将页面的R位的值加到计数器之前现将页面的计数器向右移一位，然后把页面的R位的值加到计数器的最左一位上。过改动后的计数器就能够反映过去n个时钟周期里页面的访问频率。每次淘汰计数器值最小的页。

## 分段管理

分段[最初不是用来作内存管理](https://segmentfault.com/a/1190000038810428)，而是Intel 8086为了通过16b的寄存器访问到20b的地址空间而做出的规定（将16b地址的的前12b做段基址，段基址左移四位加后4b构成20b地址），而段这一说法也就来源于这种方式下对内存的访问必须是一段一段的。

![16b分段访问覆盖20b](img/OS/16220.webp)

此时的分段地址之间并没有隔离，后来考虑到安全问题，Intel引入了**CPU保护模式**，在访问地址时，CPU会判断当前的程序是否有权访问改地址。OS利用CPU提供的该功能将程序的内存空间划分成为若干个段，每个段（如代码段、数据段、BSS段、堆、栈）都有不同的属性（段基址、段限长、段属性），利用段来保护程序。

### 相关数据设计

#### GDT 全局描述符表

整个系统中可以置于内存**任何位置**的**唯一**表，其中的表项被称为**段描述符**，GDT在内存中的地址会被设置进**GDTR**（每一个CPU核心都有一个）中。

#### 段描述符

用于描述每一个段的具体信息，其中的2bit的**DPL**（descriptor privilege level，描述符特权级）记录此段锁代表的程序在CPU中的特权级。

![断描述符内容](img/OS/segment-describal.PNG)

#### 段选择子

段选择子包括描述符索引（index）、TI、特权级（PL）三部分。描述符索引表示所需要的段的描述符在描述符表的位置，由这个位置再根据在GDTR中存储的描述符表基址就可以找到相应的描述符。段选择子中的1bit的TI值表示到哪里寻找断描述符（0在GDT，1在LDT）。2bit的特权级（PL）则代表选择子的特权级（共4个），不同段的特权级有不同的说法。

![两种段选择子](img/OS/segment-select.PNG)
### 工作机制

1. OS在启动：初始化**GDT**（表项为**段描述符**），将GDT地址设置进**GDTR**（全局描述符表寄存器  ）。
2. 程序启动时：设置**段选择子**进段寄存器。
3. 程序访存时：通过GDTR找到GDT，根据段选择子中索引到GDT中具体的**段描述符**。
4. 权限控制：将断描述符中的要访问特权级别与当前程序所拥有的特权级别作比较。

![分段寻址](img/OS/VA2PA-seg.webp)

![](img/OS/VA2PA-seg-1.webp)

### 权限保护

每当一个程序试图访问某一个段时，就将该程序所拥有的特权级（段选择子中的DPL字段）与要访问的特权级（段描述符中的级别）进行比较，以决定能否访问该段。系统约定，CPU只能访问同一特权级或级别较低特权级的段。（更详细的请参考[此处](https://blog.csdn.net/drshenlei/article/details/4265101)）

![x86的分段保护](img/OS/X86-seg-protect.PNG)

![X86模式控制环](img/OS/protect-ring.PNG)

事实上**段保护功能几乎没什么用**，因为现代的内核使用扁平的地址空间（将基地址设成0，逻辑地址与线性地址一致），用户模式的段可以访问整个线形地址空间。真正有用的**内存保护发生在分页单元**中，即从线形地址转化为物理地址的时候。一个内存页就是由一个页表项（page table entry）所描述的字节块。页表项包含两个与保护有关的字段：一个超级用户标志（supervisor flag），一个读写标志（read/write flag）。超级用户标志是内核所使用的重要的x86内存保护机制。当它开启时，内存页就不能被ring 3访问了。尽管读写标志对于实施特权控制并不像前者那么重要，但它依然十分有用。当一个进程被加载后，那些存储了二进制镜像（即代码）的内存页就被标记为只读了，从而可以捕获一些指针错误，比如程序企图通过此指针来写这些内存页。这个标志还被用于在调用fork创建Unix子进程时，实现写时拷贝功能（[copy on write](http://en.wikipedia.org/wiki/Copy-on-write)）。

![扁平模式下的内存访问](img/OS/5.PNG)

# 文件管理

## 基本概念

### 超级块

存放于磁盘的特定扇区中用于存储文件系统的控制信息（文件系统的状态、类型、大小、区块数、索引节点数等）的数据结构。

### 索引节点 INode

用于存储文件的元数据（诸如文件的大小、拥有者、创建时间、磁盘位置等和文件相关的信息）的一个数据结构。

### 文件

一组在逻辑上具有完整意义的信息项的系列。

## 进程对文件的管理

进程通过Task_Struct结构中的files_struct域来了解它当前所打开的文件对象，所谓文件描述符就是文件在进程的已打开的文件对象数组中的下标索引，文件对象通过内部域f_dentry找到文件目录项对象，通过文件对象的f_op域得到该文件支持的标准方法。

+ 超级块、Task_Struct等结构的关系如下：![relation](/run/media/linbird/79a60176-120e-4b69-b805-79ebe446d5e9/linbird/Londa/img/OS/relationship.jpg)
mmap()

<<<<<<< HEAD
# Linux Kernel

## 设计哲学

### [一切皆文件](https://cloud.tencent.com/developer/article/1512391)

+ 在Linux系统中，由于管道文件、socket文件等特殊文件的存在，一切皆文件退化为**一切皆文件描述符**。
+ bash再处理到`/dev/tcp/host/port`的充电向时建立了一个`host:port`的socket连接，将socket的读写表现的和普通文件的读写一样，但是上述的文件在文件系统并不是真实存在的，知识bash对用户的一个善意的谎言。
+ plan9系统承诺彻底贯彻执行一切皆文件，将分布在不同位置的所有资源作为文件统一在同一棵目录树中，实现Unix最初的愿景。

## 虚拟文件系统VFS

一个操作系统可以支持多种底层不同的文件系统（比如NTFS, FAT,  ext3,  ext4），为了给内核和用户进程提供统一的文件系统视图，Linux在用户进程和底层文件系统之间加入了一个抽象层，即虚拟文件系统(Virtual  File System, VFS)。通过虚拟文件系统VFS提供的抽象层来适配各种底层不同的文件系统甚至不同介质，进程所有的文件操作都由VFS完成实际的文件操作。文件IO流程简化为：

1. 应用程序通过文件操作函数（`open()、close()、read()、write()、ioctl()`）调用VFS提供的系统调用函数接口(`sys_open()、sys_close()、sys_read()、sys_write()、sys_ioctl()`)同VFS进行交互。
2. VFS通过驱动程序提供的`file_operation`接口同设备驱动进行交互（驱动层的`file_operations`方法的屏蔽了不同类型设备的底层操作方法的差异）

![虚拟文件系统层](img/OS/VFS.JPEG)

### 数据结构

VFS主要通过四个主要的结构体实现抽象层，每个结构体包含了指向该结构体支持的方法列表的指针。

![VFS中超级块、挂载点以及文件系统的关系](/run/media/linbird/79a60176-120e-4b69-b805-79ebe446d5e9/linbird/Londa/img/OS/vfs.jpg)

#### 超级块对象 super block

存储一个已安装的**文件系统的控制信息**，代表一个已安装的文件系统；每次一个实际的文件系统被安装时，内核会从磁盘的特定位置读取一些控制信息来填充**常驻内存**中的超级块对象。一个安装实例和对应一个超级块对象。

#### 目录项对象

为了方便查找文件而创建的对象，存储的是这个目录下的所有的文件的inode号和文件名等信息。其内部是树形结构，操作系统检索一个文件，都是从根目录开始，按层次解析路径中的所有目录，直到定位到文件。

#### 文件对象

**已打开的文件**在内存中的表示，主要用于建立进程和磁盘上的文件的对应关系。文件对象和物理文件的关系类型进程和程序的关系，文件对象仅仅在进程观点上代表已经打开的文件。**一个文件对应的文件对象可能不是惟一的**，但是其对应的索引节点和目录项对象是惟一的。

**file_operations**：一系列函数指针的集合，其中包含所有可以使用的系统调用函数（例如`open、read、write、mmap`等）。每个打开文件（打开文件列表模块的一个表项）都可以连接到`file_operations`模块，从而对任何已打开的文件，通过系统调用函数，实现各种操作。

**address_space**：表示一个文件在页缓存中已经缓存了的物理页。它是页缓存和外部设备中文件系统的桥梁，**关联了内存系统和文件系统**。

#### 索引节点对象 inide

存储了文件的相关信息，代表了存储设备上的一个实际的**物理文件**。当一个文件被访问时，内核会在内存中组装相应的索引节点对象，以便向内核提供对一个文件进行操作时所必需的全部信息（这些信息一部分存储在磁盘特定位置，另外一部分是在加载时动态填充的）。

#### NOTE

1. Linux支持的文件系统无论是否有文件系统的实例存在，都有且仅有一个`file_system_type`结构用于描述具体的文件系统的类型信息。相同文件系统的多个实例的超级块通过其域内的s_instances成员链接。
2. 每一个文件系统的实例都对应有一个超级块和安装点，超级块通过它的一个域s_type指向其对应的具体的文件系统类型`file_system_type`。

### 进程与VFS

![VFS内部的组织逻辑](img/OS/how-organize.png)

内核通过进程的`task_struct`中的`files`域指针找到**`file_struct`结构体**，该结构体包含了一个由`file *`构成的**已打开文件描述符表**，表中的每一个指针指向VFS中文件列表中的**文件对象**。

### 特殊文件系统

| 文件系统 | 作用 | buc  |
| :------: | :--: | :--: |
|  tmpfs   |      |      |
|  sysfs   |      |      |
| debugfs  |      |      |
|  procfs  |      |      |
|  sockfs  |      |      |
|  devfs   |      |      |

**sockfs**：socketfs伪文件系统被编译进内核（而非一个模块）在系统运行期间**总是被装载**着的（因为要支持整个TCP/IP协议栈）。它实现了VFS中的4种主要对象：超级块super block、索引节点inode、目录项对象dentry和文件对象file，当执行文件IO系统调用时，VFS就将请求转发给sockfs，而sockfs就调用具体的协议实现。

## 任务调度

Linux将所有的执行实体都称之为任务Task（Task是进程概念在Linux中的实现），由`Task_Struct`进行描述。每一个**Task都具有内存空间、执行实体、文件资源等进程都具有的资源**，从表现形式上看类似于一个单线程的进程。同时Linux允许多个任务**共享内存空间**（在`Task_Struct`对应域中指明**共享的资源空间**即可），从而使多个任务运行在同一个内存空间上。从表现上来看，此时的多个任务相当于多个线程，多个这样的线程构成了一个进程。

### Linux线程

在linux2.6之前，内核并不支持线程的概念，仅通过轻量级进程LWP模拟线程，一个用户线程对应一个内核线程（内核轻量级进程），这种模型最大的特点是线程调度由内核完成了，而其他线程操作（同步、取消）等都是核外的线程库（LinuxThread）函数完成的。[参考来源](https://developer.aliyun.com/article/374623)

在linux2.6之后，为了完全兼容posix标准，linux2.6对内核进行改进，引入了线程组的概念（仍然用轻量级进程表示线程），有了这个概念就可以将一组线程组织称为一个进程。通过这个改变，linux内核正式支持多线程特性。在实现上主要的改变就是在task_struct中加入tgid字段，这个字段就是用于表示线程组id的字段。在用户线程库方面，也使用NPTL代替LinuxThread。[参考来源](https://developer.aliyun.com/article/374623)

#### 共享的资源

内存地址、空间进程基础信息、大部分数据、打开的文件、信号处理、当前工作目录用户和用户组属性等

#### 线程独有

线程ID、寄存器、栈的局部变量、返回地址、错误码errno、信号掩码、优先级等

### 多级页表

32bit Linux采用了3级页表$[PGD(16b)|PMD(4b)|PTE(4b)|Offset(12b)]$。64bit采用了4级页表$[PG_{lobal}D|PU_{pper}D|PM_{iddle}D|PTE|Offset]$。

### 抢占内核

在Linux 2.6以前，内核只支持用户态抢占，内核态代码会一直运行直到代码被完成或者被阻塞(系统调用可以被阻塞)。Linux从2.6开始支持可抢占式内核，kernel可以在任何时间点（因为中断造成的抢占可以发生在任何时间）上抢占一个任务（要求此时间点的内核代码不持有锁处于临界区且内核代码[可重入](https://www.wikiwand.com/zh-hans/%E5%8F%AF%E9%87%8D%E5%85%A5)），从中断处理例程返回到内核态时，kernel会检查是否可以抢占和是否需要重新调度。[参考1](https://github.com/IMCG/-/blob/master/kernel/Linux%E5%86%85%E6%A0%B8%E6%80%81%E6%8A%A2%E5%8D%A0%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90.md)

### CFS调度算法



## 文件IO

为了满足速度与容量的需求，现代计算机系统都采用了多级金字塔的存储结构。在该结构中上层一般作为下层的Cache层来使用。而狭义的文件IO聚焦于Local Disk的访问特性和其与DRAM之间的数据交互。

![存储器的金字塔结构](img/OS/storage-arch.png)
<center class="half">
    <center mg src="img/OS/storage-arch.png" title="a" width="300" alt="图片说明1"/> 题注</center>
一个简单的用户态的`stdio::printf()`将经过运新模式切换、缓存切换等多个过程才能将更改内容写入存储介质：**用户态的IO函数都有自己在用户态建立的`buffer`**，这主要是出于性能的考虑（系统调用的代价是昂贵的，没必要对每一次IO都使用系统调用，尤其是小的IO，通过用户态缓冲可以**将多次IO请求聚合成一次内核IO**）。同时下图中有意忽略了存储介质自带的缓存（由介质自己管理），图中的`Kernel buffer cache`也被习惯性称之为`Page Cache`；

![缓存与IO](img/OS/io-step.png)

###  内核IO栈

![Linux内核的IO栈的结构](img/OS/Linux-storage-stack.png)

![Linux的三种文件IO模型](img/OS/linux-io-model.png)

### 标准IO

即`Buffer IO`，大多数文件系统的默认IO（如`printf(), puts()`）都采用的是标准IO的方式，需要经历**物理设备<–>设备自带缓存<–>内核缓冲区（Page Cache）<–>（用户缓冲区）<–>用户程序**的过程，其中用户缓冲区即上文提到的`stdio`等程序库提供的自实现缓冲。对于写过程一般函数只实现到由用户缓冲到内核缓冲（`write back`机制），至于**何时写入设备缓冲由OS决定、pdflush(page dirty flush)内核线程执行**（可以调用`sync`等干预），由设备缓存到设备由设备自身控制。

**优点**：隔离用户和内核地址空间以加强安全；汇聚IO请求以减少硬件请求；

**缺点**：数据在用户和内核之间的多次拷贝带来的CPU和内存开销；**延迟写**机制可能造成数据丢失

**场景**：适用于大多数普通文件操作，对性能、吞吐量没有特殊要求，由kernel通过page cache统一管理缓存。默认是异步写，如果使用sync则是同步写。

### [内存映射](#内存映射：`mmap()`)

### 直接IO

不经过内核缓冲区直接访问介质数据（此时由用户程序**自己设计提供缓冲机制**，通常和**异步IO**结合使用来等待硬件响应）。需要经历**物理设备<–>设备自带缓存<–>（用户缓冲区）<–>用户程序**的过程

**优点**：减少了内存拷贝和一些系统调用

**缺点**：用户程序实现复杂，需要自己提供缓冲机制和与设备的IO处理

**场景**：性能要求较高、内存使用率也要求较高的情况下使用。如数据库等结合自身数据特点设计了自己高线缓存的程序。


## 系统调用

### 内存映射：`mmap()`

#### 概述

![mmap的I/O 模型](img/OS/mmap-model.jpg)

系统调用mmap可以将文件映射至内存(进程空间)，如此可以把对文件的操作转为对内存的操作，以此避免更多的lseek()、read()、write()等系统调用，这点对于大文件或者频繁访问的文件尤其有用，提高了I/O效率。

`mmap()`将一个文件（一切皆文件中的文件）或者其它对象映射到进程的地址空间，实现了文件地址和进程虚拟地址的映射关系。实现映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用read，write等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如下图所示：

本质上来讲， **mmap 实现的是内核缓冲区与用户进程的地址空间的映射**，也就是说用户进程通过操作自己的逻辑虚拟地址就可以实现操作内核空间缓冲区，这样就不用再因为内核空间和用户空间相互隔离而需要将数据在内核缓冲区和用户进程所在内存之间来回拷贝。

mmap 负责映射文件逻辑上一段连续的数据（物理上可以不连续存储）映射为连续内存，而这里的文件可以是磁盘文件、驱动假造出的文件（例如 DMA 技术）以及设备；

#### 函数原型

```c
#include <sys/mman.h>;
void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);//分配内存映射区
int munmap(void *addr, size_t length);//释放内存映射区
```

##### [参数及注意事项](http://www.yushuai.xyz/2019/05/31/4365.html)
**`addr`**: 建立映射区的首地址，使用时，直接传递`NULL`，实际参数由Linux内核确定。

**`length`**：欲创建映射区的大小，不能创建0字节大小的映射区。

**`prot`**：映射区权限：`PROT_READ、PROT_WRITE、PROT_EXEC、PROT_NONE`支持或操作组合。映射区创建过程中对文件有一次**隐含的读操作**；创建的映射区的权限要**小于等于**打开文件的权限。

**`flags`**：标志位参数(常用于设定更新物理区域、设置共享、创建匿名映射区)，支持或操作组合。

|       位        |                             含义                             |
| :-------------: | :----------------------------------------------------------: |
|  `MAP_SHARED`   | 对映射区的修改会影响其他同方式的进程，更改会被写回物理设备上 |
|  `MAP_PRIVATE`  |      映射区所做的修改不会影响其他进程，不会写回物理设备      |
| `MAP_ANONYMOUS` |       匿名映射，映射区不与任何文件关联（`fd`必须为-1）       |

**fd**：用来建立映射区的文件描述符（创建完后**可以先关闭**），-1为匿名映射，否则为文件映射。

**offset**：映射文件的偏移（必须是**4k的整数倍**，因为映射是由MMU帮忙创建的，而MMU操作的基本单位就是4K），可以把整个文件都映射到内存空间，也可以只把一部分文件内容映射到内存空间。

##### 返回值

成功返回创建的映射区首地址、失败返回`MAP_FAILED`宏。但是并不是以返回值开始`len`长的地址都可访问，只有**部分可访问**。文件大小、 `mmap`的参数 `len` 都不能决定进程能访问的大小，容纳文件被映射部分的最小页面数决定了进程能访问的大小。

![mmap()可访问地址范围](img/OS/accessable.png)

#### [过程分析](https://www.jianshu.com/p/57fc5833387a)

1. 内核在进程的**虚拟地址空间**内找到满足大小`len`的**连续地址空间**（可以超过页），为此空间创建并设置一个**新的`vm_area_struct`**结构，并将此结构体加入到**`mm_struct`**引出的链表或树中。
2.
   1. 如果是**文件映射**：则通过**文件描述符**在内核的**已打开文件表**索引到**文件结构体**，通过其**`file_operation`**域调用该文件**驱动提供的`int mmap(struct file *filp, struct vm_area_struct *vma)`函数**，该函数会找到文件的**`inode`定位**到硬盘地址，然后通过[内存映射函数`remap_pfn_range`](https://www.cnblogs.com/pengdonglin137/p/8149859.html)**建立页表项**（）实现了文件地址和虚拟地址区域的映射关系（**此时虚拟地址并没有关联到主存主存，即没有分配物理内存**）。
   2. 如果是**匿名映射**：
3. 进程发起对这片映射空间的访问，引发**缺页异常**，内核处理缺页（分配物理内存并调入页）。页面成为了**脏页**后系统会**延时等待**一段时间后将页面**回写**到磁盘地址。

#### 优点

1. 减少系统调用
2. 减少内存拷贝
3. 可靠性高：延迟写

#### 缺点

1. 占用虚拟内存增大
2. 延迟：mmap 通过缺页中断向磁盘发起真正的磁盘 I/O

#### 适用场景、、、、、、、、、、、、、、

|              |     私有映射     |           共享映射           |       内容       |
| :----------: | :--------------: | :--------------------------: | :--------------: |
| **匿名映射** |  常用于内存分配  |  共享内存用于进程间共享内存  |    初始化为0     |
| **文件映射** | 常用于加载动态库 | 常用于内存映射IO，进程间通信 | 被映射的部分文件 |

**进程通信**：`mmap` 由内核负责管理，对同一个文件地址的映射将被所有线程共享，因为进程可以相当于直接读写内存，因此是**最快的IPC形式**（相比于管道和消息队列等避免了许多数据拷贝和系统调用）。操作系统确保线程安全以及线程可见性；

**快速IO**：适用于对文件操作的吞吐量、性能有一定要求，且对内存使用不敏感，不要求实时写回的情况下使用。

### 内存分配 `brk()`

brk是传统分配/释放堆内存的系统调用, 堆内存是由低地址向高地址方向增长;

- 分配内存时,将数据段(.data)的最高地址指针_edata往高地址扩展;
- 释放内存时,把_edata向低地址收缩。

可以看出brk系统调用管理的始终是一片连续的虚拟地址空间，而且起始地址一经设定就默认不变，只是高地址按需变化。

## 进程通信

### 管道

### 共享内存

实际上，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。

#### Socket

#### 消息队列

#### 信号量

## 内存布局

### 物理地址布局

![4GB下的物理内存布局](img/OS/phisics-memory-layout.jpg)

#### ZONE_DMA（0~16M）

由于**DMA不经过MMU直接使用物理内存**地址访问内存，而且**需要连续的缓冲区**，所以必须为DMA划分一段连续的物理地址空间。因此内核在物理内存中开辟专门的ZONE_DMA区域**专门供I/O设备的DMA**使用。此区域也被称为**内存空洞**，CPU对该区域地址的**访问请求不会到达内存而会被自动转发到相应的IO设备**。

#### ZONE_NORMAL（16M~896M）

内核**能够直接使用**。

#### ZONE_HIGHMEM（896M~结束）

内核**不能直接使用**此区域的内容。

### 物理地址与虚拟地址

Linux简化了分段机制（Intel由于历史原因必须支持分段）~~，使得虚拟地址与线性地址总是一致~~。在32bit Linux中，内核将最高的1GB虚拟内存空间作为内核空间（**所有的进程共享内核空间地址**），对内核空间的访问将受到硬件保护(0级才可访问)，低3GB虚拟内存空间作为用户空间（包含代码段、全局变量、BSS、函数栈、堆内存、映射区等）供用户进程使用（3级可访问）。

![Linux内核空间布局](./img/OS/memory-layout.png)

![共享内核空间](img/OS/user-kernal-space.png)

![Linux物理地址与虚拟地址的映射](img/OS/PA-VA.jpg)

、、、、、、、、、、、、、、、、、、、、、

由于开启了分页机制，内核想要访问物理地址空间的话，必须先建立映射关系，然后通过虚拟地址来访问。为了能够访问所有的物理地址空间，就要将全部物理地址空间映射到1G的内核线性空间中，这显然不可能。于是，内核将0~896M的物理地址空间一对一映射到自己的线性地址空间中，这样它便可以随时访问ZONE_DMA和ZONE_NORMAL里的物理页面；此时内核剩下的128M线性地址空间不足以完全映射所有的ZONE_HIGHMEM，Linux采取了动态映射的方法，即按需的将ZONE_HIGHMEM里的物理页面映射到kernel  space的最后128M线性地址空间里，使用完之后释放映射关系，以供其它物理页面映射。虽然这样存在效率的问题，但是内核毕竟可以正常的访问所有的物理地址空间了。

、、、、、、、、、、、、、、、、、、、、、

### 内核空间布局

该段虚拟地址空间对应的物理内存**常驻内存**，不会被OS换出到磁盘等设备。



由于开启了分页机制，内核想要访问物理地址空间的话，必须先建立映射关系，然后通过虚拟地址来访问。为了能够访问所有的物理地址空间，就要将全部物理地址空间映射到1G的内核线性空间中，这显然不可能。于是，内核将0~896M的物理地址空间一对一映射到自己的线性地址空间中，这样它便可以随时访问ZONE_DMA和ZONE_NORMAL里的物理页面；此时内核剩下的128M线性地址空间不足以完全映射所有的ZONE_HIGHMEM，Linux采取了动态映射的方法，即按需的将ZONE_HIGHMEM里的物理页面映射到kernel  space的最后128M线性地址空间里，使用完之后释放映射关系，以供其它物理页面映射。虽然这样存在效率的问题，但是内核毕竟可以正常的访问所有的物理地址空间了。

![内核地址空间分布](img/OS/kernel-space-layout.jpg)

#### 低16MB

对应着物理地址空间中的ZONE_DMA区域。

#### 中880MB

此段对应着物理地址空间中的ZONE_NORMAL区域，内核会将**频繁使用的数据**（内核代码、GDT、IDT、PGD、mem_map数组等）放置在此段**直接使用**。

**直接映射区**：

#### 高128MB

此段内也叫高端内存（若机器安装的物理内存超过内核地址空间范围，就会存在高端内存，高端内存只和逻辑地址有关系），存放用户数据？？？？？？、页表(PT)等不常用数据。

而将用户数据、页表(PT)等不常用数据放在ZONE_ HIGHMEM里，只在要访问这些数据时才建立映射关系(kmap())。比如，当内核要访问I/O设备存储空间时，就使用ioremap()将位于物理地址高端的mmio区内存映射到内核空间的vmalloc area中，在使用完之后便断开映射关系。[来源]（https://blog.csdn.net/farmwang/article/details/66976818?utm_source=debugrun&utm_medium=referral）



**动态内存映射区**：

**永久内存映射区**：

**`vmalloc`内存区**：

### [用户空间段布局](https://www.cnblogs.com/fuzhe1989/p/3936894.html)

|  段分布  |                             内容                             | 分配方式 |           大小           | 运行态 |
| :------: | :----------------------------------------------------------: | :------: | :----------------------: | :----: |
|  保留段  |   0x08048000(x86下)：以下为了便于**检查空指针**的一段空间    |   静态   |         固定大小         |  用户  |
|  代码段  |    程序指令、**字符串常量**、虚函数表（**只读**、可共享）    |   静态   |        编译时确定        |  用户  |
|  数据段  |                  初始化的全局变量和静态变量                  |   静态   |        编译时确定        |  用户  |
|   BSS    |            未初始化的全局变量和静态变量（全为0）             |   静态   |        编译时确定        |  用户  |
|  映射段  |              动态链接库、共享文件、匿名映射对象              |   动态   |           动态           |  用户  |
|    堆    | 动态申请的数据、有**碎片**问题、[`brk()`函数](https://blog.csdn.net/shuzishij/article/details/86574927) |   动态   |       和栈共享上限       |  用户  |
|    栈    | 局部变量、函数参数与返回值、函数返回地址、调用者环境信息（如寄存器值） |   静态   | 和堆共享上限、**可设置** |  用户  |
| 内核空间 |      储操作系统、驱动程序、**命令行参数和环境变量**等……      |  动+静   |           定长           |  内核  |

![程序段分布](img/OS/segments-layout.jpg)

Linux通过将每个段的起始地址赋予一个随机偏移量**`random offset`**来打乱内存布局（否则进程内存布局缺乏变化，容易被试探出内存布局）以加强安全性。

**代码段、数据段**：大小和内容在程序被编译后就被固定了在了目标文件之中，通常都被存储在预分配的连续内存之中。

**BSS段（Block Started by Symbol）**：大小在编译时固定（目标文件仅记录所需的大小，无实际数据），**实际数据只存在于内存之中**，内存中的内容由OS全部初始化为0。

**映射段（Memory Mapping Segment）**：Linux中通过`mmap()`系统调用，Windows中通过`creatFileMapping()/MapViewOfFile()`**动态创建**的区域。内核通过使用该区域（**内存映射**）可以避免内核空间和用户空间的文件拷贝，直接将文件内容直接映射到内存以实现**快速IO**。常被用来加载动态库、创建[匿名映射](https://www.jianshu.com/p/b24265a3a222)。在C库函数中，如果一次申请内存大于`M_MMAP_THRESHOLD(128K)`字节时，库函数会**自动使用映射段创建匿名映射**。

**栈**：Linux进程在运行过程中遇到栈满时会自动试图增加栈大小（上限是`RLIMIT_STACK(8MB)`），如果无法增加才会出现段错误。而只有[动态栈增长](http://lxr.linux.no/linux+v2.6.28.1/arch/x86/mm/fault.c#L692)（由`alloca()`完成）访问到未分配区域是合法的（白色区域）其它方式访问到未映射区域时都会引发一次页错误，进而导致段错误。

**内核空间**：内核空间和用户空间**不能简单地使用指针传递数据**。

### 段的管理

进程的多个段的连续地址空间构成多个独立的内存区域。Linux内核使用**`vm_area_struct`结构**（包含区域起始和终止地址、指向该区域支持的系统调用函数集合的**`vm_ops`指针**）来表示一个独立的虚拟内存区域，一个进程拥有的多个`vm_area_struct`将被链接接起来方便进程访问。

![Linux进程内存管理](img/OS/vm_area_struct.png)

# 上下文

所谓的**上下文**指的是一个运行环境(对于计算机就是一系列的变量值)。

## 进程上下文

当前进程上下文均保存在**进程的任务数据结构**中，一个进程的上下文环境具体可以分为**用户级上下文，寄存器上下文和系统级上下文**三大的部分。

|     层级     |                             内容                             |
| :----------: | :----------------------------------------------------------: |
| 用户级上下文 |             正文、数据、用户堆栈以及共享存储区等             |
| 寄存器上下文 | 通用寄存器、程序寄存器(IP)、处理器状态寄存器(EFLAGS)、栈指针(ESP)等 |
| 系统级上下文 | 进程控制块task_struct、内存管理信息(mm_struct、vm_area_struct、pgd、pte)、内核栈等 |

## 中断上下文

为了快速响应硬件的事件，对同一个 CPU 来说**中断处理比进程拥有更高的优先级**，硬件通过触发中断信号，此时**中断处理会打断进程的正常调度和执行**，转而进入内核空间调用中断处理程序（一般都短小精悍且**不会阻塞**，以便尽可能快的执行结束），响应设备事件。

这个过程中传递的硬件中断参数和内核需要保存的一些其他环境（主要是保存被打断执行的**部分进程环境**使进程在中断结束后得以恢复）就被称作**中断上下文**（只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等）。

![中断上下文](img/OS/int-process.jpg)

## 上下文切换

CPU在执行任务时必须依赖外部环境信息（上下文），而每个CPU单元某一时间点只能处于一个确切的环境下，OS通过分时复用不断执行多个进程，造成宏观上的并行。CPU每次执行不同的任务时，它所处的环境也会发生改变，这个改变的过程就是上下文切换，通过上下文切换CPU得以正确运行。

### 模式上下文切换

用户态代码进行系统调用时发生的用户态-内核态-用户态的切换过程，这一过程改变了运行的特权模式，主要的切换内容为**寄存器上下文的切换**。

### 进程上下文切换

进程是由内核来管理和调度的，因此进程的切换**只发生在内核态**。进程一旦被调度将会发生整个进程**三种上下文的整体切换**（保存上一进程、加载下一进程），进程切换过程中OS必须切换虚地址空间（页表的加载等），在新进程的运行过程中不断刷新替换TLB中的旧内容（**缓存失效**会加大访存时间），这一过程的代价相当大（几十纳秒到数微秒）。

![进程的上下文切换](./img/OS/process-switch,jpg)

### 线程上下文切换

由于线程很多资源（虚拟内存和全局变量等资源）都是共享进程的，所以切换时只需要切换线程的私有数据，切换代价比进程切换小得多。

### 中断上下文切换

中断上下文切换只**发生在内核态**，中断时，内核不代表任何进程运行、不涉及到进程的用户态、一般只访问系统空间。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。

## 上下文与运行态

核心态代码（系统调用、中断处理程序等）拥有完全的底层资源控制权限，可以执行任何CPU指令，访问任何内存地址。用户态不能直接访问底层硬件和内存地址，用户态运行的程序必须委托系统调用来访问硬件和内存。

内核态和用户态限定的当前代码所能执行的指令集合和访问地址空间，上下文决定了代码执行时的环境。由此二者可以确定CPU上任何一个任务所属的状态只可能时以下三种之一：

| 状态 / 空间 |     上下文     |    栈使用    |                本体                |
| :---------: | :------------: | :----------: | :--------------------------------: |
|   内核态    | 处于进程上下文 | 用进程内核栈 |         代表某个特定的进程         |
|   内核态    | 处于中断上下文 | 用进程内核栈 | 与任何进程无关，处理某个特定的中断 |
|   用户态    | 处于进程上下文 | 用进程用户栈 |          代表某个用户进程          |

当一个进程通过**系统调用、中断、软中断**（如缺页异常）从用户态陷入内核态，此时会**使用当前进程的内核栈**代表进程执行（**多个进程共享内核空间**，但是每个进程进入内核的上下文不同，因此**每个进程都有自己的内核栈**），整体处于进程上下文中。
